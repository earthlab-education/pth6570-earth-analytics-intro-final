{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "import io\n",
    "import math \n",
    "import os\n",
    "import pathlib\n",
    "from glob import glob\n",
    "\n",
    "import earthpy as et\n",
    "import earthpy.appeears as etapp\n",
    "import earthpy.earthexplorer as etee\n",
    "import geopandas as gpd\n",
    "import hvplot.xarray  \n",
    "import hvplot.pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import rioxarray as rxr\n",
    "import rioxarray.merge as rxrmerge\n",
    "import requests\n",
    "import xarray as xr\n",
    "import zipfile\n",
    "from shapely.geometry import box\n",
    "from xrspatial import slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data directory\n",
    "data_dir = os.path.join(et.io.HOME, et.io.DATA_NAME, 'final')\n",
    "\n",
    "if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "\n",
    "# Make data directory the working directory\n",
    "os.chdir(data_dir)\n",
    "\n",
    "# Define utm zone\n",
    "utm = 32613"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from: https://medium.com/@loldja/reading-shapefile-zips-from-a-url-in-python-3-93ea8d727856\n",
    "# Create directory\n",
    "grassland_url = ('https://data.fs.usda.gov/geodata/edw/'\n",
    "                 'edw_resources/shp/S_USA.NationalGrassland.zip'\n",
    ")\n",
    "print('Downloading shapefile...')\n",
    "\n",
    "# Request data from url\n",
    "grassland_request = requests.get(grassland_url)\n",
    "grassland_zip = zipfile.ZipFile(io.BytesIO(grassland_request.content))\n",
    "print(\"Done\")\n",
    "\n",
    "# Extract files from Zip to \n",
    "grassland_zip.extractall(\n",
    "    path=os.path.join(data_dir, 'national-grassland')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import shapefile\n",
    "grassland_gdf = gpd.read_file(os.path.join(\n",
    "    data_dir, 'national-grassland', 'S_USA.NationalGrassland.shp')\n",
    "    )\n",
    "\n",
    "select_grassland_gdf = (\n",
    "    grassland_gdf\n",
    "    .set_index('GRASSLANDN')\n",
    "    .loc[['Comanche National Grassland', 'Pawnee National Grassland']]\n",
    ")\n",
    "\n",
    "select_grassland_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_polaris_data(data_directory, input_gdf, index_col_name):\n",
    "    \"\"\"\n",
    "    This function downloads in Polaris soil data for the extent of each \n",
    "    row in a geodataframe and creates a merged data array. Downloaded \n",
    "    data is saved in individual folders for each row. Merged data arrays\n",
    "    for each row are saved in \"Merged_files\" subfolder.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    data_directory : path\n",
    "        The path to the data directory. A sub-directory will be created\n",
    "        within this directory for the soil data downloads. \n",
    "\n",
    "    input_gdf: geopandas.GeoDataFrame\n",
    "        A geodataframe that contains the areas of interest. Polaris soil\n",
    "        data will be downloaded according to the bounds of each row in \n",
    "        the dataframe. \n",
    "    \n",
    "    index_col_name: string\n",
    "        A string containing the name of the geoDataFrame column that\n",
    "        should be used as the index. This index will be used to name\n",
    "        output files.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    output_lst: list of data arrays\n",
    "        Returns a list of data arrays representing the merged Polaris\n",
    "        tif data for each row in the input geodataframe\n",
    "    \"\"\"\n",
    "\n",
    "    # Set index to specified index column\n",
    "    input_gdf = input_gdf.reset_index().set_index(index_col_name)\n",
    "\n",
    "    # Create dataframe of bounds of each gdf row\n",
    "    bound_pd = pd.concat([input_gdf.bounds], axis=1)\n",
    "\n",
    "    file_list = []\n",
    "    \n",
    "    # Loop through each row in the boundary dataframe\n",
    "    for ind in bound_pd.index:\n",
    "        print(\"\\n\", ind, \":\")\n",
    "        # Define and round min and max longitude and latitudes\n",
    "        min_lon = math.floor(bound_pd['minx'][ind])\n",
    "        max_lon = math.ceil(bound_pd['maxx'][ind])\n",
    "        min_lat = math.floor(bound_pd['miny'][ind])\n",
    "        max_lat = math.ceil(bound_pd['maxy'][ind])\n",
    "        # Define range\n",
    "        lat_range = range(min_lat,max_lat)\n",
    "        lon_range = range(min_lon,max_lon)\n",
    "\n",
    "        # Create template for polaris url path\n",
    "        polaris_template_url = 'http://hydrology.cee.duke.edu/POLARIS/PROPERTIES/v1.0/' \\\n",
    "                    '{0}/{1}/{2}/lat{3}{4}_lon{5}{6}.tif'\n",
    "        # Create template for file names\n",
    "        polaris_template_name = 'mean_ph_lat{0}{1}_lon{2}{3}.tif'\n",
    "\n",
    "        # Create sub-directory for soil data\n",
    "        soil_dir = os.path.join(data_directory, \"soil_data\")\n",
    "        if not os.path.exists(soil_dir):\n",
    "            os.makedirs(soil_dir)\n",
    "\n",
    "        # Create sub-folders for each row's data\n",
    "        soil_row_dir = os.path.join(soil_dir, str(ind).replace(\" \", \"_\"))\n",
    "        if not os.path.exists(soil_row_dir):\n",
    "            os.makedirs(soil_row_dir)\n",
    "        \n",
    "        # Create sub-folder for merged data arrays\n",
    "        soil_merged_dir = os.path.join(soil_dir, \"Merged_tifs\")\n",
    "        if not os.path.exists(soil_merged_dir):\n",
    "            os.makedirs(soil_merged_dir)\n",
    "        \n",
    "        # For each latitude and longitude in the extent, download file\n",
    "        for lat in lat_range:\n",
    "            for lon in lon_range:\n",
    "                # Define url for each data file in extent\n",
    "                url = polaris_template_url.format('ph',\n",
    "                                        'mean',\n",
    "                                        '60_100',\n",
    "                                        str(lat),\n",
    "                                        str(lat+1),\n",
    "                                        str(lon),\n",
    "                                        str(lon+1))\n",
    "                # Define file name for each data file in extent\n",
    "                file_name = polaris_template_name.format(str(lat),\n",
    "                                        str(lat+1),\n",
    "                                        str(lon),\n",
    "                                        str(lon+1))\n",
    "                # Check if tif file is in directory. Download if not.\n",
    "                file_name_path = os.path.join(soil_row_dir, file_name)\n",
    "                if not os.path.exists(file_name_path): \n",
    "                    print(file_name, \"does not exist. Downloading file\")\n",
    "                    r = requests.get(url, allow_redirects=True)\n",
    "                    open(file_name_path, 'wb').write(r.content)\n",
    "                else:\n",
    "                    print(file_name, \"is already downloaded\")\n",
    "\n",
    "        # Merge arrays if the merged file does not exist\n",
    "        merge_template_name = '{0}_merged_da.tif'\n",
    "        merge_da_name = merge_template_name.format(str(ind).replace(\" \", \"_\"), \".tif\")\n",
    "        if not os.path.exists(os.path.join(soil_merged_dir, merge_da_name)): \n",
    "            print(\"Soil data is being merged.\")\n",
    "            tif_paths = glob(os.path.join(soil_row_dir, '*.tif'))\n",
    "            das = [rxr.open_rasterio(tif, masked=True) for tif in tif_paths]\n",
    "            merged_da = rxrmerge.merge_arrays(das)\n",
    "            merged_da.rio.to_raster(\n",
    "                os.path.join(soil_merged_dir, merge_da_name)\n",
    "                )\n",
    "        else:\n",
    "            print(\"A merged soil data array already exists.\")\n",
    "            merged_da = rxr.open_rasterio(\n",
    "                os.path.join(soil_merged_dir, merge_da_name), masked=True\n",
    "                )\n",
    "        \n",
    "        # Add file to list of tif files to return\n",
    "        file_list.append(os.path.join(soil_merged_dir, merge_da_name))\n",
    "        \n",
    "    # Return files\n",
    "    output_lst = [rxr.open_rasterio(tif, masked=True).squeeze() for tif in file_list]\n",
    "    return output_lst\n",
    "\n",
    "comanche_pH_da, pawnee_pH_da = get_polaris_data(data_dir, select_grassland_gdf, \"GRASSLANDN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change projection of grassland data\n",
    "select_grassland_utm_gdf = select_grassland_gdf.to_crs(utm)\n",
    "\n",
    "# Clip files to bounds\n",
    "pawnee_pH_da = (pawnee_pH_da\n",
    "                  .rio.reproject(utm)\n",
    "                  .rio.clip_box(*select_grassland_utm_gdf\n",
    "                                .bounds\n",
    "                                .loc['Pawnee National Grassland'])\n",
    ")\n",
    "\n",
    "comanche_pH_da = (comanche_pH_da\n",
    "                  .rio.reproject(utm)\n",
    "                  .rio.clip_box(*select_grassland_utm_gdf\n",
    "                                .loc[['Comanche National Grassland']]\n",
    "                                .total_bounds)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
